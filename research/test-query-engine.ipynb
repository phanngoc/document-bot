{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page: 3518\n",
      "page: 3519\n",
      "page: 3520\n",
      "page: 3521\n",
      "page: 3522\n",
      "page: 3523\n",
      "page: 3524\n",
      "page: 3525\n",
      "page: 3526\n",
      "page: 3527\n",
      "page: 3528\n",
      "page: 3529\n",
      "page: 3530\n",
      "page: 3531\n",
      "page: 3532\n",
      "page: 3533\n",
      "page: 3534\n",
      "page: 3535\n",
      "page: 3536\n",
      "page: 3537\n",
      "page: 3538\n",
      "page: 3539\n",
      "page: 3540\n",
      "page: 3541\n",
      "page: 3542\n",
      "page: 3543\n",
      "page: 3544\n",
      "page: 3545\n",
      "page: 3546\n",
      "page: 3547\n",
      "page: 3548\n",
      "page: 3549\n",
      "page: 3550\n",
      "page: 3551\n",
      "page: 3552\n",
      "page: 3553\n",
      "page: 3554\n",
      "page: 3555\n",
      "page: 3556\n",
      "page: 3557\n",
      "page: 3558\n",
      "page: 3559\n",
      "page: 3560\n",
      "page: 3561\n",
      "page: 3562\n",
      "page: 3563\n",
      "page: 3564\n",
      "page: 3565\n",
      "page: 3566\n",
      "page: 3567\n",
      "page: 3568\n",
      "page: 3569\n",
      "page: 3570\n",
      "page: 3571\n",
      "page: 3572\n",
      "page: 3573\n",
      "page: 3574\n",
      "page: 3575\n",
      "page: 3576\n",
      "page: 3577\n",
      "page: 3578\n",
      "page: 3579\n",
      "page: 3580\n",
      "page: 3581\n",
      "page: 3582\n",
      "page: 3583\n",
      "page: 3584\n",
      "page: 3585\n",
      "page: 3586\n",
      "page: 3587\n",
      "page: 3588\n",
      "page: 3589\n",
      "page: 3590\n",
      "page: 3591\n",
      "page: 3592\n",
      "page: 3593\n",
      "page: 3594\n",
      "page: 3595\n",
      "page: 3596\n",
      "page: 3597\n",
      "page: 3598\n",
      "page: 3599\n",
      "page: 3600\n",
      "page: 3601\n",
      "page: 3602\n",
      "page: 3603\n",
      "page: 3604\n",
      "page: 3605\n",
      "page: 3606\n",
      "page: 3607\n",
      "page: 3608\n",
      "page: 3609\n",
      "page: 3610\n",
      "page: 3611\n",
      "page: 3612\n",
      "page: 3613\n",
      "page: 3614\n",
      "page: 3615\n",
      "page: 3616\n",
      "page: 3617\n",
      "page: 3618\n",
      "page: 3619\n",
      "page: 3620\n",
      "page: 3621\n",
      "page: 3622\n",
      "page: 3623\n",
      "page: 3624\n",
      "page: 3625\n",
      "page: 3626\n",
      "page: 3627\n",
      "page: 3628\n",
      "page: 3629\n",
      "page: 3630\n",
      "page: 3631\n",
      "page: 3632\n",
      "page: 3633\n",
      "page: 3634\n",
      "page: 3635\n",
      "page: 3636\n",
      "page: 3637\n",
      "page: 3638\n",
      "page: 3639\n",
      "page: 3640\n",
      "page: 3641\n",
      "page: 3642\n",
      "page: 3643\n",
      "page: 3644\n",
      "page: 3645\n",
      "page: 3646\n",
      "page: 3647\n",
      "page: 3648\n",
      "page: 3649\n",
      "page: 3650\n",
      "page: 3651\n",
      "page: 3652\n",
      "page: 3653\n",
      "page: 3654\n",
      "page: 3655\n",
      "page: 3656\n",
      "page: 3657\n",
      "page: 3658\n",
      "page: 3659\n",
      "page: 3660\n",
      "page: 3661\n",
      "page: 3662\n",
      "page: 3663\n",
      "page: 3664\n",
      "page: 3665\n",
      "page: 3666\n",
      "page: 3667\n",
      "page: 3668\n",
      "page: 3669\n",
      "page: 3670\n",
      "page: 3671\n",
      "page: 3672\n",
      "page: 3673\n",
      "page: 3674\n",
      "page: 3675\n",
      "page: 3676\n",
      "page: 3677\n",
      "page: 3678\n",
      "page: 3679\n",
      "page: 3680\n",
      "page: 3681\n",
      "page: 3682\n",
      "page: 3683\n",
      "page: 3684\n",
      "page: 3685\n",
      "page: 3686\n",
      "page: 3687\n",
      "page: 3688\n",
      "page: 3689\n",
      "page: 3690\n",
      "page: 3691\n",
      "page: 3692\n",
      "page: 3693\n",
      "page: 3694\n",
      "page: 3695\n",
      "page: 3696\n",
      "page: 3697\n",
      "page: 3698\n",
      "page: 3699\n",
      "page: 3700\n",
      "page: 3701\n",
      "page: 3702\n",
      "page: 3703\n",
      "page: 3704\n",
      "page: 3705\n",
      "page: 3706\n",
      "page: 3707\n",
      "page: 3708\n",
      "page: 3709\n",
      "page: 3710\n",
      "page: 3711\n",
      "page: 3712\n",
      "page: 3713\n",
      "page: 3714\n",
      "page: 3715\n",
      "page: 3716\n",
      "page: 3717\n",
      "page: 3718\n",
      "page: 3719\n",
      "page: 3720\n",
      "page: 3721\n",
      "page: 3722\n",
      "page: 3723\n",
      "page: 3724\n",
      "page: 3725\n",
      "page: 3726\n",
      "page: 3727\n",
      "page: 3728\n",
      "page: 3729\n",
      "page: 3730\n",
      "page: 3731\n",
      "page: 3732\n",
      "page: 3733\n",
      "page: 3734\n",
      "page: 3735\n",
      "page: 3736\n",
      "page: 3737\n",
      "page: 3738\n",
      "page: 3739\n",
      "page: 3740\n",
      "page: 3741\n",
      "page: 3742\n",
      "page: 3743\n",
      "page: 3744\n",
      "page: 3745\n",
      "page: 3746\n",
      "page: 3747\n",
      "page: 3748\n",
      "page: 3749\n",
      "page: 3750\n",
      "page: 3751\n",
      "page: 3752\n",
      "page: 3753\n",
      "page: 3754\n",
      "page: 3755\n",
      "page: 3756\n",
      "page: 3757\n",
      "page: 3758\n",
      "page: 3759\n",
      "page: 3760\n",
      "page: 3761\n",
      "page: 3762\n",
      "page: 3763\n",
      "page: 3764\n",
      "page: 3765\n",
      "page: 3766\n",
      "page: 3767\n",
      "page: 3768\n",
      "page: 3769\n",
      "page: 3770\n",
      "page: 3771\n",
      "page: 3772\n",
      "page: 3773\n",
      "page: 3774\n",
      "page: 3775\n",
      "page: 3776\n",
      "page: 3777\n",
      "page: 3778\n",
      "page: 3779\n",
      "page: 3780\n",
      "page: 3781\n",
      "page: 3782\n",
      "page: 3783\n",
      "page: 3784\n",
      "page: 3785\n",
      "page: 3786\n",
      "page: 3787\n",
      "page: 3788\n",
      "page: 3789\n",
      "page: 3790\n",
      "page: 3791\n",
      "page: 3792\n",
      "page: 3793\n",
      "page: 3794\n",
      "page: 3795\n",
      "page: 3796\n",
      "page: 3797\n",
      "page: 3798\n",
      "page: 3799\n",
      "page: 3800\n",
      "page: 3801\n",
      "page: 3802\n",
      "page: 3803\n",
      "page: 3804\n",
      "page: 3805\n",
      "page: 3806\n",
      "page: 3807\n",
      "page: 3808\n",
      "page: 3809\n",
      "page: 3810\n",
      "page: 3811\n",
      "page: 3812\n",
      "page: 3813\n",
      "page: 3814\n",
      "page: 3815\n",
      "page: 3816\n",
      "page: 3817\n",
      "page: 3818\n",
      "page: 3819\n",
      "page: 3820\n",
      "page: 3821\n",
      "page: 3822\n",
      "page: 3823\n",
      "page: 3824\n",
      "page: 3825\n",
      "page: 3826\n",
      "page: 3827\n",
      "page: 3828\n",
      "page: 3829\n",
      "page: 3830\n",
      "page: 3831\n",
      "page: 3832\n",
      "page: 3833\n",
      "page: 3834\n",
      "page: 3835\n",
      "page: 3836\n",
      "page: 3837\n",
      "page: 3838\n",
      "page: 3839\n",
      "page: 3840\n",
      "page: 3841\n",
      "page: 3842\n",
      "page: 3843\n",
      "page: 3844\n",
      "page: 3845\n",
      "page: 3846\n",
      "page: 3847\n",
      "page: 3848\n",
      "page: 3849\n",
      "page: 3850\n",
      "page: 3851\n",
      "page: 3852\n",
      "page: 3853\n",
      "page: 3854\n",
      "page: 3855\n",
      "page: 3856\n",
      "page: 3857\n",
      "page: 3858\n",
      "page: 3859\n",
      "page: 3860\n",
      "page: 3861\n",
      "page: 3862\n",
      "page: 3863\n",
      "page: 3864\n",
      "page: 3865\n",
      "page: 3866\n",
      "page: 3867\n",
      "page: 3868\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.core import StorageContext\n",
    "from llama_index.core import Document\n",
    "from model import Assistant, Page\n",
    "import openai\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()  # Load environment variables from a .env file\n",
    "\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "\n",
    "documents = []\n",
    "assistant = Assistant.get(5)\n",
    "\n",
    "pages = Page.select().where(Page.assistant == assistant)\n",
    "\n",
    "for page in pages:\n",
    "    print('page:', page.id)\n",
    "    doc = Document(\n",
    "        id=str(page.id) if page.id else None,\n",
    "        text=page.text_content\n",
    "    )\n",
    "    documents.append(doc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "laravel.com\n",
      "docs.streamlit.io\n",
      "docs.scrapy.org\n",
      "python-rq.org\n",
      "platform.openai.com\n",
      "developer.vonage.com\n"
     ]
    }
   ],
   "source": [
    "from chromadb.config import Settings\n",
    "from chromadb import Client\n",
    "\n",
    "# Configure persistent storage for Chroma\n",
    "sqlite_settings = Settings(\n",
    "    persist_directory=\"./chroma_data\",  # Directory for storing SQLite files\n",
    "    is_persistent = True\n",
    ")\n",
    "chroma_client = Client(settings=sqlite_settings)\n",
    "\n",
    "# List all collections\n",
    "collections = chroma_client.list_collections()\n",
    "for collection in collections:\n",
    "    print(collection.name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|██████████| 351/351 [00:02<00:00, 138.53it/s]\n",
      "Generating embeddings: 100%|██████████| 2048/2048 [00:54<00:00, 37.81it/s]\n"
     ]
    }
   ],
   "source": [
    "collection_name = 'laravel.com'\n",
    "collection = chroma_client.get_collection(collection_name)\n",
    "\n",
    "# Setup vector store and storage context\n",
    "vector_store = ChromaVectorStore(chroma_collection=collection)\n",
    "\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents, storage_context, show_progress=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response: Scrapy is an application framework used for crawling websites and extracting structured data, which can be utilized for various purposes such as data mining, information processing, or historical archival. It was originally designed for web scraping but can also extract data through APIs or function as a general-purpose web crawler.\n"
     ]
    }
   ],
   "source": [
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"What is scrapy ?.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response: Scrapy is an application framework used for crawling websites and extracting structured data, which can be utilized for various purposes such as data mining, information processing, or historical archival. It was originally designed for web scraping but can also extract data through APIs or function as a general-purpose web crawler. <class 'llama_index.core.base.response.schema.Response'>\n"
     ]
    }
   ],
   "source": [
    "print('response:', response, type(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NodeWithScore(node=TextNode(id_='b673ca07-27d9-4a59-ba1d-6fa5eeb377e7', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='79b33721-771c-4575-abe6-fdc6e4e11a9d', node_type='4', metadata={}, hash='c338efc66e1edf7f55d5a5fd0103c4e3c6066efeeeadbf9ac1b0e1a4904caa91'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='7458d7b2-f5a2-4021-9b53-f82078744fd1', node_type='1', metadata={}, hash='55102aa8d69ec90fd29d30dbf3cdfd35c810d206b87ce15dd6123022686a7426')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='.. _intro-overview: ================== Scrapy at a glance ==================\\nScrapy (/ˈskreɪpaɪ/) is an application framework for crawling web sites and\\nextracting structured data which can be used for a wide range of useful\\napplications, like data mining, information processing or historical archival.\\nEven though Scrapy was originally designed for `web scraping`_, it can also be\\nused to extract data using APIs (such as `Amazon Associates Web Services`_) or\\nas a general purpose web crawler. Walk-through of an example spider\\n================================= In order to show you what Scrapy brings to\\nthe table, we\\'ll walk you through an example of a Scrapy Spider using the\\nsimplest way to run a spider. Here\\'s the code for a spider that scrapes famous\\nquotes from website https://quotes.toscrape.com, following the pagination: ..\\ncode-block:: python import scrapy class QuotesSpider(scrapy.Spider): name =\\n\"quotes\" start_urls = [ \"https://quotes.toscrape.com/tag/humor/\", ] def\\nparse(self, response): for quote in response.css(\"div.quote\"): yield {\\n\"author\": quote.xpath(\"span/small/text()\").get(), \"text\":\\nquote.css(\"span.text::text\").get(), } next_page = response.css(\\'li.next\\na::attr(\"href\")\\').get() if next_page is not None: yield\\nresponse.follow(next_page, self.parse) Put this in a text file, name it\\nsomething like ``quotes_spider.py`` and run the spider using the\\n:command:`runspider` command:: scrapy runspider quotes_spider.py -o\\nquotes.jsonl When this finishes you will have in the ``quotes.jsonl`` file a\\nlist of the quotes in JSON Lines format, containing the text and author, which\\nwill look like this:: {\"author\": \"Jane Austen\", \"text\": \"\\\\u201cThe person, be\\nit gentleman or lady, who has not pleasure in a good novel, must be\\nintolerably stupid.\\\\u201d\"} {\"author\": \"Steve Martin\", \"text\": \"\\\\u201cA day\\nwithout sunshine is like, you know, night.\\\\u201d\"} {\"author\": \"Garrison\\nKeillor\", \"text\": \"\\\\u201cAnyone who thinks sitting in church can make you a\\nChristian must also think that sitting in a garage can make you a car.\\\\u201d\"}\\n... What just happened? \\\\------------------- When you ran the command ``scrapy\\nrunspider quotes_spider.py``, Scrapy looked for a Spider definition inside it\\nand ran it through its crawler engine. The crawl started by making requests to\\nthe URLs defined in the ``start_urls`` attribute (in this case, only the URL\\nfor quotes in the *humor* category) and called the default callback method\\n``parse``, passing the response object as an argument. In the ``parse``\\ncallback, we loop through the quote elements using a CSS Selector, yield a\\nPython dict with the extracted quote text and author, look for a link to the\\nnext page and schedule another request using the same ``parse`` method as\\ncallback. Here you will notice one of the main advantages of Scrapy: requests\\nare :ref:`scheduled and processed asynchronously `. This means that Scrapy\\ndoesn\\'t need to wait for a request to be finished and processed, it can send\\nanother request or do other things in the meantime. This also means that other\\nrequests can keep going even if a request fails or an error happens while\\nhandling it. While this enables you to do very fast crawls (sending multiple\\nconcurrent requests at the same time, in a fault-tolerant way) Scrapy also\\ngives you control over the politeness of the crawl through :ref:`a few\\nsettings `. You can do things like setting a download delay between each\\nrequest, limiting the amount of concurrent requests per domain or per IP, and\\neven :ref:`using an auto-throttling extension ` that tries to figure these\\nsettings out automatically. .. note:: This is using :ref:`feed exports ` to\\ngenerate the JSON file, you can easily change the export format (XML or CSV,\\nfor example) or the storage backend (FTP or `Amazon S3`_, for example). You\\ncan also write an :ref:`item pipeline ` to store the items in a database. ..\\n_topics-whatelse: What else? ========== You\\'ve seen how to extract and store\\nitems from a website using Scrapy, but this is just the surface.', mimetype='text/plain', start_char_idx=0, end_char_idx=4018, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=0.7769685568968078)]\n"
     ]
    }
   ],
   "source": [
    "print(response.source_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _intro-overview: ================== Scrapy at a glance ==================\n",
      "Scrapy (/ˈskreɪpaɪ/) is an application framework for crawling web sites and\n",
      "extracting structured data which can be used for a wide range of useful\n",
      "applications, like data mining, information processing or historical archival.\n",
      "Even though Scrapy was originally designed for `web scraping`_, it can also be\n",
      "used to extract data using APIs (such as `Amazon Associates Web Services`_) or\n",
      "as a general purpose web crawler. Walk-through of an example spider\n",
      "================================= In order to show you what Scrapy brings to\n",
      "the table, we'll walk you through an example of a Scrapy Spider using the\n",
      "simplest way to run a spider. Here's the code for a spider that scrapes famous\n",
      "quotes from website https://quotes.toscrape.com, following the pagination: ..\n",
      "code-block:: python import scrapy class QuotesSpider(scrapy.Spider): name =\n",
      "\"quotes\" start_urls = [ \"https://quotes.toscrape.com/tag/humor/\", ] def\n",
      "parse(self, response): for quote in response.css(\"div.quote\"): yield {\n",
      "\"author\": quote.xpath(\"span/small/text()\").get(), \"text\":\n",
      "quote.css(\"span.text::text\").get(), } next_page = response.css('li.next\n",
      "a::attr(\"href\")').get() if next_page is not None: yield\n",
      "response.follow(next_page, self.parse) Put this in a text file, name it\n",
      "something like ``quotes_spider.py`` and run the spider using the\n",
      ":command:`runspider` command:: scrapy runspider quotes_spider.py -o\n",
      "quotes.jsonl When this finishes you will have in the ``quotes.jsonl`` file a\n",
      "list of the quotes in JSON Lines format, containing the text and author, which\n",
      "will look like this:: {\"author\": \"Jane Austen\", \"text\": \"\\u201cThe person, be\n",
      "it gentleman or lady, who has not pleasure in a good novel, must be\n",
      "intolerably stupid.\\u201d\"} {\"author\": \"Steve Martin\", \"text\": \"\\u201cA day\n",
      "without sunshine is like, you know, night.\\u201d\"} {\"author\": \"Garrison\n",
      "Keillor\", \"text\": \"\\u201cAnyone who thinks sitting in church can make you a\n",
      "Christian must also think that sitting in a garage can make you a car.\\u201d\"}\n",
      "... What just happened? \\------------------- When you ran the command ``scrapy\n",
      "runspider quotes_spider.py``, Scrapy looked for a Spider definition inside it\n",
      "and ran it through its crawler engine. The crawl started by making requests to\n",
      "the URLs defined in the ``start_urls`` attribute (in this case, only the URL\n",
      "for quotes in the *humor* category) and called the default callback method\n",
      "``parse``, passing the response object as an argument. In the ``parse``\n",
      "callback, we loop through the quote elements using a CSS Selector, yield a\n",
      "Python dict with the extracted quote text and author, look for a link to the\n",
      "next page and schedule another request using the same ``parse`` method as\n",
      "callback. Here you will notice one of the main advantages of Scrapy: requests\n",
      "are :ref:`scheduled and processed asynchronously `. This means that Scrapy\n",
      "doesn't need to wait for a request to be finished and processed, it can send\n",
      "another request or do other things in the meantime. This also means that other\n",
      "requests can keep going even if a request fails or an error happens while\n",
      "handling it. While this enables you to do very fast crawls (sending multiple\n",
      "concurrent requests at the same time, in a fault-tolerant way) Scrapy also\n",
      "gives you control over the politeness of the crawl through :ref:`a few\n",
      "settings `. You can do things like setting a download delay between each\n",
      "request, limiting the amount of concurrent requests per domain or per IP, and\n",
      "even :ref:`using an auto-throttling extension ` that tries to figure these\n",
      "settings out automatically. .. note:: This is using :ref:`feed exports ` to\n",
      "generate the JSON file, you can easily change the export format (XML or CSV,\n",
      "for example) or the storage backend (FTP or `Amazon S3`_, for example). You\n",
      "can also write an :ref:`item pipeline ` to store the items in a database. ..\n",
      "_topics-whatelse: What else? ========== You've seen how to extract and store\n",
      "items from a website using Scrapy, but this is just the surface.\n"
     ]
    }
   ],
   "source": [
    "for node_with_score in response.source_nodes:\n",
    "    document_node = node_with_score.node\n",
    "    print(document_node.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docs.scrapy.org\n",
      "platform.openai.com\n"
     ]
    }
   ],
   "source": [
    "# Delete the collection named 'platform_openai_com'\n",
    "collection_to_delete = 'python-rq.org'\n",
    "chroma_client.delete_collection(collection_to_delete)\n",
    "\n",
    "# Verify deletion by listing all collections again\n",
    "collections = chroma_client.list_collections()\n",
    "for collection in collections:\n",
    "    print(collection.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of elements in the collection 'laravel.com': 3701\n"
     ]
    }
   ],
   "source": [
    "collection_to_count = 'laravel.com'\n",
    "collection = chroma_client.get_collection(collection_to_count)\n",
    "element_count = collection.count()\n",
    "print(f\"Number of elements in the collection '{collection_to_count}':\", element_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_name = \"quickstart\"\n",
    "chroma_collection = chroma_client.get_or_create_collection(collection_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|██████████| 20/20 [00:34<00:00,  1.71s/it] \n",
      "Generating embeddings: 100%|██████████| 846/846 [00:28<00:00, 30.10it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Setup vector store and storage context\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "\n",
    "# Setup vector store and storage context\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents, storage_context, show_progress=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response: To use HuggingFace LLM - StableLM, you can follow these steps:\n",
      "\n",
      "1. Install the required packages by running the following command:\n",
      "```python\n",
      "%pip install llama-index-llms-huggingface\n",
      "```\n",
      "\n",
      "2. Import the necessary modules:\n",
      "```python\n",
      "import logging\n",
      "import sys\n",
      "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
      "from llama_index.llms.huggingface import HuggingFaceLLM\n",
      "from llama_index.core import Settings\n",
      "```\n",
      "\n",
      "3. Set up the prompts specific to StableLM:\n",
      "```python\n",
      "from llama_index.core import PromptTemplate\n",
      "\n",
      "system_prompt = \"\"\"<|SYSTEM|># StableLM Tuned (Alpha version)\n",
      "- StableLM is a helpful and harmless open-source AI language model developed by StabilityAI.\n",
      "- StableLM is excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n",
      "- StableLM is more than just an information source, StableLM is also able to write poetry, short stories, and make jokes.\n",
      "- StableLM will refuse to participate in anything that could harm a human.\n",
      "\"\"\"\n",
      "\n",
      "query_wrapper_prompt = PromptTemplate(\"<|USER|>{query_str}<|ASSISTANT|>\")\n",
      "```\n",
      "\n",
      "These steps will help you set up and use HuggingFace LLM - StableLM with the necessary configurations and prompts.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "response = query_engine.query(\"how to use HuggingFace LLM - StableLM ? write example code\")\n",
    "print('response:', response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
